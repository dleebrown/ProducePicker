{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"reads in a saved xgboost model and predicts prices for the 3 months following the last date contained in\n",
    "some input produce data\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from xgboost.sklearn import XGBRegressor  \n",
    "import pickle\n",
    "import datetime\n",
    "import math\n",
    "sns.set_style(\"darkgrid\")\n",
    "model_dir = './saved_xgb_models/'\n",
    "data_dir = './trimmed_data_and_plots/'\n",
    "predict_dir = './produce_predictions/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_stored_model(city, veggie, directory):\n",
    "    # read in pickled xgb model\n",
    "    loaded_model = pickle.load(open(directory+city+'_'+veggie+'_model', \"rb\"))\n",
    "    return loaded_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_trimmed_data(city, veggie, data_dir):\n",
    "    # reads in cleaned .csv data for one city and veggie, returns\n",
    "    output_data = pd.read_csv(data_dir+veggie+'_'+city+'_TRIM.csv')\n",
    "    output_data.rename(columns={'Unnamed: 0': 'Date'}, inplace=True)\n",
    "    output_data['Date'] = pd.to_datetime(output_data['Date'])\n",
    "    output_data = output_data.sort_values(by='Date')\n",
    "    output_data = output_data.reset_index(drop=True)\n",
    "    #output_data = normalize_price(output_data)\n",
    "    return output_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_price(input_data): \n",
    "    # normalize produce price - ultimately not used here\n",
    "    input_data['Average Price'] = (input_data['Average Price'] - input_data['Average Price'].median())/input_data['Average Price'].std()\n",
    "    return input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nearest_date(dates, targdate):\n",
    "    # finds nearest date to targdate in list of dates\n",
    "    diff = abs(dates - targdate)\n",
    "    nearest = dates.iloc[diff.idxmin]\n",
    "    timedelta = abs(nearest - targdate)\n",
    "    return nearest, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train(veggie_data, train_length):\n",
    "    # select data to use to make the predictions for xgb\n",
    "    all_dates = pd.to_datetime(veggie_data['Date'])\n",
    "    start_date = all_dates.tail(1).iloc[0]\n",
    "    train_time = pd.to_timedelta(pd.np.ceil(train_days), unit=\"D\")\n",
    "    start_date_train = start_date - train_time\n",
    "    start_date_train = start_date_train\n",
    "    nearest_date_train, deltatrain = nearest_date(all_dates, start_date_train)\n",
    "    training_set = veggie_data[(veggie_data['Date'] >= nearest_date_train) & (veggie_data['Date'] < start_date)]\n",
    "    return training_set, start_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_features(input_data, start_date, test_days, train_days):\n",
    "    # feature engineering\n",
    "    features = []\n",
    "    # convert times to deltas\n",
    "    train_time = pd.to_timedelta(pd.np.ceil(train_days), unit=\"D\")\n",
    "    test_time = pd.to_timedelta(pd.np.ceil(test_days), unit=\"D\")\n",
    "    one_year = pd.to_timedelta(pd.np.ceil(365), unit='D')\n",
    "    ten = pd.to_timedelta(pd.np.ceil(14), unit='D')\n",
    "    twenty = pd.to_timedelta(pd.np.ceil(14), unit='D')\n",
    "    thirty = pd.to_timedelta(pd.np.ceil(30), unit='D')\n",
    "    sixty = pd.to_timedelta(pd.np.ceil(60), unit='D')\n",
    "    ninety = pd.to_timedelta(pd.np.ceil(90), unit='D')\n",
    "    recent_dates = [ten, twenty, thirty, sixty, ninety, one_year]\n",
    "    \n",
    "    # convert pricing data to pct_change\n",
    "    pct_changes = input_data['Average Price'].pct_change()\n",
    "    pct_changes = pd.concat([input_data['Date'], pct_changes], axis=1)\n",
    "    # replace 0s with nans for now\n",
    "    pct_changes['Average Price'] = pct_changes['Average Price'].replace(0, np.NaN)\n",
    "    num_years = train_days // 365\n",
    "    # compute historical averages and stds\n",
    "    for y in range(num_years):\n",
    "        historical_prices = input_data[(input_data['Date'] >= (start_date-(y+1)*one_year)) & (input_data['Date'] <= (start_date-(y+1)*one_year + test_time))]\n",
    "        features.append(historical_prices['Average Price'].mean())\n",
    "        features.append(historical_prices['Average Price'].std())\n",
    "        \n",
    "        # now append stock ticker/oil (external data):\n",
    "        features.append(historical_prices['SYY'].mean())\n",
    "        features.append(historical_prices['FDP'].mean())\n",
    "        features.append(historical_prices['SENEA'].mean())\n",
    "        features.append(historical_prices['CAG'].mean())\n",
    "        features.append(historical_prices['KR'].mean())\n",
    "        features.append(historical_prices['POILWTIUSDM'].mean())\n",
    "\n",
    "        historical_pct = pct_changes[(pct_changes['Date'] >= (start_date-(y+1)*one_year)) & (pct_changes['Date'] <= (start_date-(y+1)*one_year + test_time))]\n",
    "        features.append(historical_pct['Average Price'].mean())\n",
    "        features.append(historical_pct['Average Price'].std())\n",
    "        \n",
    "        # now append stock ticker/oil (external data):\n",
    "        features.append(historical_prices['SYY'].pct_change().mean())\n",
    "        features.append(historical_prices['FDP'].pct_change().mean())\n",
    "        features.append(historical_prices['SENEA'].pct_change().mean())\n",
    "        features.append(historical_prices['CAG'].pct_change().mean())\n",
    "        features.append(historical_prices['KR'].pct_change().mean())\n",
    "        \n",
    "    # now do last 14, 30, etc. day features avg/std\n",
    "    for d in recent_dates:\n",
    "        recent_prices = input_data[(input_data['Date'] >= (start_date - d)) & (input_data['Date'] <= (start_date))]\n",
    "        features.append(recent_prices['Average Price'].mean())\n",
    "        features.append(recent_prices['Average Price'].std())\n",
    "        features.append(recent_prices['Date'].dt.month.mean())\n",
    "        \n",
    "        # just tickers because of oil sparsity\n",
    "        features.append(recent_prices['SYY'].mean())\n",
    "        features.append(recent_prices['FDP'].mean())\n",
    "        features.append(recent_prices['SENEA'].mean())\n",
    "        features.append(recent_prices['CAG'].mean())\n",
    "        features.append(recent_prices['KR'].mean())\n",
    "        \n",
    "        recent_pct = pct_changes[(pct_changes['Date'] >= (start_date - d)) & (pct_changes['Date'] <= (start_date))]\n",
    "        features.append(recent_pct['Average Price'].mean())\n",
    "        features.append(recent_pct['Average Price'].std())\n",
    "        \n",
    "        # ticker data\n",
    "        features.append(recent_prices['SYY'].pct_change().mean())\n",
    "        features.append(recent_prices['FDP'].pct_change().mean())\n",
    "        features.append(recent_prices['SENEA'].pct_change().mean())\n",
    "        features.append(recent_prices['CAG'].pct_change().mean())\n",
    "        features.append(recent_prices['KR'].pct_change().mean())\n",
    "    return(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_corr_features(input_data, start_date, test_days, train_days):\n",
    "    # feature engineering\n",
    "    features = []\n",
    "    # convert times to deltas\n",
    "    train_time = pd.to_timedelta(pd.np.ceil(train_days), unit=\"D\")\n",
    "    test_time = pd.to_timedelta(pd.np.ceil(test_days), unit=\"D\")\n",
    "    one_year = pd.to_timedelta(pd.np.ceil(365), unit='D')\n",
    "    fourteen = pd.to_timedelta(pd.np.ceil(14), unit='D')\n",
    "    thirty = pd.to_timedelta(pd.np.ceil(30), unit='D')\n",
    "    recent_dates = [fourteen, thirty]\n",
    "    # convert pricing data to pct_change\n",
    "    pct_changes = input_data['Average Price'].pct_change()\n",
    "    pct_changes = pd.concat([input_data['Date'], pct_changes], axis=1)\n",
    "    # replace 0s with nans for now\n",
    "    pct_changes['Average Price'] = pct_changes['Average Price'].replace(0, np.NaN)\n",
    "    # compute historical averages and stds\n",
    "    num_years = train_days // 365\n",
    "    for y in range(num_years):\n",
    "        historical_prices = input_data[(input_data['Date'] >= (start_date-(y+1)*one_year)) & (input_data['Date'] <= (start_date-(y+1)*one_year + test_time))]\n",
    "        features.append(historical_prices['Average Price'].mean())\n",
    "    # now do last 14, 30, day features avg/std\n",
    "    for d in recent_dates:\n",
    "        recent_prices = input_data[(input_data['Date'] >= (start_date - d)) & (input_data['Date'] <= (start_date))]\n",
    "        features.append(recent_prices['Average Price'].mean())\n",
    "    return(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def current_price(input_data):\n",
    "    # retrieve the last week's worth of data\n",
    "    return input_data['Average Price'].tail(7).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities = ['NEW+YORK', 'LOS+ANGELES']\n",
    "veggies = ['APPLES','APRICOTS','ASPARAGUS','AVOCADOS','BANANAS','BEANS','BEETS','BLACKBERRIES','BLUEBERRIES','BROCCOLI','CABBAGE','CANTALOUPS','CARROTS','CAULIFLOWER','CELERY','CHERRIES','CLEMENTINES', 'CUCUMBERS','ENDIVE','GARLIC','GINGER+ROOT','GRAPEFRUIT','GRAPES','HONEYDEWS','KIWIFRUIT','LEMONS','LETTUCE%2C+ICEBERG','LETTUCE%2C+ROMAINE','LETTUCE%2C+RED+LEAF','LETTUCE%2C+GREEN+LEAF', 'LIMES','NECTARINES','OKRA','ORANGES','PEACHES','PEARS','PEAS+GREEN','PEPPERS%2C+BELL+TYPE','PINEAPPLES','PLUMS','POTATOES','RADISHES','RASPBERRIES','RHUBARB','SPINACH','SQUASH','STRAWBERRIES']\n",
    "train_days = 365*3\n",
    "\n",
    "test_days = 90\n",
    "\n",
    "# load the correlation_data\n",
    "correlation_dir = './'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "APPLES\n",
      "APRICOTS\n",
      "ASPARAGUS\n",
      "AVOCADOS\n",
      "BANANAS\n",
      "BEANS\n",
      "BEETS\n",
      "BLACKBERRIES\n",
      "BLUEBERRIES\n",
      "BROCCOLI\n",
      "BRUSSELS+SPROUTS\n",
      "CABBAGE\n",
      "CANTALOUPS\n",
      "CARROTS\n",
      "CAULIFLOWER\n",
      "CELERY\n",
      "CHERRIES\n",
      "CLEMENTINES\n",
      "CUCUMBERS\n",
      "ENDIVE\n",
      "GARLIC\n",
      "GINGER+ROOT\n",
      "GRAPEFRUIT\n",
      "GRAPES\n",
      "HONEYDEWS\n",
      "KIWIFRUIT\n",
      "LEMONS\n",
      "LETTUCE%2C+ICEBERG\n",
      "LETTUCE%2C+ROMAINE\n",
      "LETTUCE%2C+RED+LEAF\n",
      "LETTUCE%2C+GREEN+LEAF\n",
      "LIMES\n",
      "MANGOES\n",
      "NECTARINES\n",
      "OKRA\n",
      "ORANGES\n",
      "PEACHES\n",
      "PEARS\n",
      "PEAS+GREEN\n",
      "PEPPERS%2C+BELL+TYPE\n",
      "PINEAPPLES\n",
      "PLUMS\n",
      "POTATOES\n",
      "RADISHES\n",
      "RASPBERRIES\n",
      "RHUBARB\n",
      "SPINACH\n",
      "SQUASH\n",
      "STRAWBERRIES\n",
      "TURNIPS\n",
      "APPLES\n",
      "APRICOTS\n",
      "ASPARAGUS\n",
      "AVOCADOS\n",
      "BANANAS\n",
      "BEANS\n",
      "BEETS\n",
      "BLACKBERRIES\n",
      "BLUEBERRIES\n",
      "BROCCOLI\n",
      "BRUSSELS+SPROUTS\n",
      "CABBAGE\n",
      "CANTALOUPS\n",
      "CARROTS\n",
      "CAULIFLOWER\n",
      "CELERY\n",
      "CHERRIES\n",
      "CLEMENTINES\n",
      "CUCUMBERS\n",
      "ENDIVE\n",
      "GARLIC\n",
      "GINGER+ROOT\n",
      "GRAPEFRUIT\n",
      "GRAPES\n",
      "HONEYDEWS\n",
      "KIWIFRUIT\n",
      "LEMONS\n",
      "LETTUCE%2C+ICEBERG\n",
      "LETTUCE%2C+ROMAINE\n",
      "LETTUCE%2C+RED+LEAF\n",
      "LETTUCE%2C+GREEN+LEAF\n",
      "LIMES\n",
      "MANGOES\n",
      "NECTARINES\n",
      "OKRA\n",
      "ORANGES\n",
      "PEACHES\n",
      "PEARS\n",
      "PEAS+GREEN\n",
      "PEPPERS%2C+BELL+TYPE\n",
      "PINEAPPLES\n",
      "PLUMS\n",
      "POTATOES\n",
      "RADISHES\n",
      "RASPBERRIES\n",
      "RHUBARB\n",
      "SPINACH\n",
      "SQUASH\n",
      "STRAWBERRIES\n",
      "TURNIPS\n"
     ]
    }
   ],
   "source": [
    "labels = ['Item', 'Current Price', '3 Month Prediction']\n",
    "# city loop\n",
    "\n",
    "for c in cities:\n",
    "    \n",
    "    correlation_list = pd.read_csv(correlation_dir+c+'_correlations.csv')\n",
    "\n",
    "    outputs = []\n",
    "    # loop over veggies and make a prediction for each\n",
    "    for v in veggies:\n",
    "\n",
    "        input_data = read_trimmed_data(c, v, data_dir)\n",
    "        vcorrs = correlation_list[v+'_norm']\n",
    "        historical_data, sdate = get_train(input_data, train_days)\n",
    "\n",
    "        inputX = build_features(historical_data, sdate, test_days, train_days, vcorrs, c, data_dir)\n",
    "        # catch excepts with a valuerror?\n",
    "        currentprice = current_price(historical_data)\n",
    "\n",
    "        model = read_stored_model(c, v, model_dir)\n",
    "\n",
    "        outputY = model.predict(inputX)\n",
    "        print(v)\n",
    "\n",
    "        outputs.append((v, currentprice, outputY[0]))\n",
    "    all_predictions = pd.DataFrame.from_records(outputs, columns=labels)\n",
    "\n",
    "    all_predictions.to_csv(predict_dir + c + '_predictions.csv')\n",
    "\n",
    "    # end city loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
